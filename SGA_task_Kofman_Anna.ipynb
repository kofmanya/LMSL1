{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User routes on the site\n",
    "## Description\n",
    "**Clickstream** is a sequence of user actions on a website. It allows you to understand how users interact with the site. In this task, you need to find the most frequent custom routes.\n",
    "\n",
    "## Input data\n",
    "Input data is а table with clickstream data in file `hdfs:/data/clickstream.csv`.\n",
    "\n",
    "### Table structure\n",
    "* `user_id (int)` - Unique user identifier.\n",
    "* `session_id (int)` - Unique identifier for the user session. The user's session lasts until the identifier changes.\n",
    "* `event_type (string)` - Event type from the list:\n",
    "    * **page** - visit to the page\n",
    "    * **event** - any action on the page\n",
    "    * <b>&lt;custom&gt;</b> - string with any other type\n",
    "* `event_type (string)` - Page on the site.\n",
    "* `timestamp (int)` - Unix-timestamp of action.\n",
    "\n",
    "### Browser errors\n",
    "Errors can sometimes occur in the user's browser - after such an error appears, we can no longer trust the data of this session and all the following lines after the error or at the same time with it are considered corrupted and **should not be counted** in statistics.\n",
    "\n",
    "When an error occurs on the page, a random string containing the word **error** will be written to the `event_type` field.\n",
    "\n",
    "### Sample of user session\n",
    "<pre>\n",
    "+-------+----------+------------+----------+----------+\n",
    "|user_id|session_id|  event_type|event_page| timestamp|\n",
    "+-------+----------+------------+----------+----------+\n",
    "|    562|       507|        page|      main|1620494781|\n",
    "|    562|       507|       event|      main|1620494788|\n",
    "|    562|       507|       event|      main|1620494798|\n",
    "|    562|       507|        page|    family|1620494820|\n",
    "|    562|       507|       event|    family|1620494828|\n",
    "|    562|       507|        page|      main|1620494848|\n",
    "|    562|       507|wNaxLlerrorU|      main|1620494865|\n",
    "|    562|       507|       event|      main|1620494873|\n",
    "|    562|       507|        page|      news|1620494875|\n",
    "|    562|       507|        page|   tariffs|1620494876|\n",
    "|    562|       507|       event|   tariffs|1620494884|\n",
    "|    562|       514|        page|      main|1620728918|\n",
    "|    562|       514|       event|      main|1620729174|\n",
    "|    562|       514|        page|   archive|1620729674|\n",
    "|    562|       514|        page|     bonus|1620729797|\n",
    "|    562|       514|        page|   tariffs|1620731090|\n",
    "|    562|       514|       event|   tariffs|1620731187|\n",
    "+-------+----------+------------+----------+----------+\n",
    "</pre>\n",
    "\n",
    "#### Correct user routes for a given user:\n",
    "* **Session 507**: main-family-main\n",
    "* **Session 514**: main-archive-bonus-tariffs\n",
    "\n",
    "Route elements are ordered by the time they appear in the clickstream, from earliest to latest.\n",
    "\n",
    "The route must be accounted for completely before the end of the session or an error in the session.\n",
    "\n",
    "## Task\n",
    "You need to use the Spark SQL, Spark RDD and Spark DF interfaces to create a solution file, the lines of which contain **the 30 most frequent user routes** on the site.\n",
    "\n",
    "Each line of the file should contain the `route` and `count` values **separated by tabs**, where:\n",
    "* `route` - route on the site, consisting of pages separated by \"-\".\n",
    "* `count` - the number of user sessions in which this route was.\n",
    "\n",
    "The lines must be **ordered in descending order** of the `count` field.\n",
    "\n",
    "## Criteria\n",
    "You can get maximum of 3.5 points (final grade) for this assignment, depedning on the number of interface you manage to leverage. The criteria are as follows:\n",
    "\n",
    "* 0.5 points – Spark SQL solution with 1 query\n",
    "* 0.5 points – Spark SQL solution with <=2 queries\n",
    "* 0.5 points – Spark RDD solution\n",
    "* 0.5 points – Spark DF solution\n",
    "* 0.5 points – your solution algorithm is relatively optimized, i.e.: no O^2 or O^3 complexities; appropriate object usage; no data leaks etc. This is evaluated by staff.\n",
    "* 1 point – 1 on 1 screening session. During this session staff member can ask you questions regarding your solution logic, framework usage, questionable parts of your code etc. If your code is clean enough, the staff member can just ask you to solve a theoretical problem connected to Spark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Spark SQL Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2024-10-13 15:04:15,278 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName='jupyter')\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "se = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copyFromLocal: `clickstream.csv': File exists\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -copyFromLocal clickstream.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "drwxr-xr-x   - jovyan supergroup          0 2024-10-13 15:04 .sparkStaging\n",
      "-rw-r--r--   1 jovyan supergroup   32241574 2024-10-06 12:03 clickstream.csv\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------+----------+----------+\n",
      "|user_id|session_id|  event_type|event_page| timestamp|\n",
      "+-------+----------+------------+----------+----------+\n",
      "|    562|       507|        page|      main|1695584127|\n",
      "|    562|       507|       event|      main|1695584134|\n",
      "|    562|       507|       event|      main|1695584144|\n",
      "|    562|       507|       event|      main|1695584147|\n",
      "|    562|       507|wNaxLlerrorU|      main|1695584154|\n",
      "|    562|       507|       event|      main|1695584154|\n",
      "|    562|       507|       event|      main|1695584154|\n",
      "|    562|       507|       event|      main|1695584160|\n",
      "|    562|       507|        page|    rabota|1695584166|\n",
      "|    562|       507|       event|    rabota|1695584174|\n",
      "|    562|       507|       event|    rabota|1695584181|\n",
      "|    562|       507|       event|    rabota|1695584189|\n",
      "|    562|       507|        page|      main|1695584194|\n",
      "|    562|       507|       event|      main|1695584204|\n",
      "|    562|       507|       event|      main|1695584211|\n",
      "|    562|       507|       event|      main|1695584211|\n",
      "|    562|       507|       event|      main|1695584219|\n",
      "|    562|       507|        page|     bonus|1695584221|\n",
      "|    562|       507|        page|    online|1695584222|\n",
      "|    562|       507|       event|    online|1695584230|\n",
      "+-------+----------+------------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = se.read.csv('clickstream.csv', header=True, sep='\\t')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.registerTempTable(\"df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main\t8184\n",
      "main-archive\t1113\n",
      "main-rabota\t1047\n",
      "main-internet\t897\n",
      "main-bonus\t870\n",
      "main-news\t769\n",
      "main-tariffs\t677\n",
      "main-online\t587\n",
      "main-vklad\t518\n",
      "main-rabota-archive\t170\n",
      "main-archive-rabota\t167\n",
      "main-bonus-archive\t143\n",
      "main-rabota-bonus\t139\n",
      "main-bonus-rabota\t135\n",
      "main-news-rabota\t135\n",
      "main-archive-internet\t132\n",
      "main-rabota-news\t130\n",
      "main-internet-rabota\t129\n",
      "main-archive-news\t126\n",
      "main-rabota-internet\t124\n",
      "main-internet-archive\t123\n",
      "main-archive-bonus\t117\n",
      "main-internet-bonus\t115\n",
      "main-tariffs-internet\t114\n",
      "main-news-archive\t113\n",
      "main-news-internet\t109\n",
      "main-archive-tariffs\t104\n",
      "main-internet-news\t103\n",
      "main-tariffs-archive\t103\n",
      "main-rabota-main\t94\n"
     ]
    }
   ],
   "source": [
    "most_frequent_routes_with_cte = se.sql('''\n",
    "with session_limit as (\n",
    "    select user_id, session_id, min(timestamp) as first_error_timestamp\n",
    "    from df\n",
    "    where event_type LIKE '%error%'\n",
    "    group by user_id, session_id\n",
    "),\n",
    "\n",
    "ordered_events as (\n",
    "    select d.user_id, d.session_id, d.event_page, d.timestamp,\n",
    "           lag(d.event_page) over (partition by d.user_id, d.session_id order by d.timestamp) as prev_page\n",
    "    from df d\n",
    "    left join session_limit sl on sl.user_id = d.user_id and sl.session_id = d.session_id\n",
    "    where d.event_type = 'page'\n",
    "      and (sl.first_error_timestamp is null or d.timestamp < sl.first_error_timestamp)\n",
    "),\n",
    "\n",
    "filtered_events as (\n",
    "    select user_id, session_id, event_page, timestamp\n",
    "    from ordered_events\n",
    "    where event_page != prev_page or prev_page is null\n",
    "),\n",
    "\n",
    "all_routes as (\n",
    "    select user_id, session_id, concat_ws('-', collect_list(event_page)) as route\n",
    "    from (\n",
    "        select user_id, session_id, event_page, timestamp\n",
    "        from filtered_events\n",
    "        order by user_id, session_id, timestamp\n",
    "    )\n",
    "    group by user_id, session_id\n",
    ")\n",
    "\n",
    "select route, count(distinct user_id, session_id) as cnt\n",
    "from all_routes\n",
    "group by route\n",
    "order by cnt desc, route\n",
    "limit 30\n",
    "''')\n",
    "\n",
    "most_frequent_routes_spark_sql = most_frequent_routes_with_cte.collect()\n",
    "\n",
    "for row in most_frequent_routes_spark_sql:\n",
    "    print(f\"{row['route']}\\t{row['cnt']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Let's include a logic explanation here:**\n",
    "\n",
    "1. CTE **session_limit**: \n",
    "- Finds the first error for each user session. \n",
    "- For each *(user_id, session_id)* pair, the query calculates the minimum *timestamp*, which represents the first occurence of an error in that session.\n",
    "- This CTE helps us determine when to stop considering page visits for the session.\n",
    "\n",
    "2. CTE **ordered_events**: \n",
    "- Captures valid 'page' events for each session and introduces a *lag()* to track the previous page within the same session. \n",
    "- We join the *df* DataFrame with the *session_limit* to include the first error *timestamp* for each session. \n",
    "- We then filter the data to include only rows where *event_type* = 'page'. \n",
    "- The query further filters events to include those that happened before the first error. \n",
    "- The *lag()* function creates a new column *prev_page* that contains the previous page visited in the session. \n",
    "- It uses a window function *(PARTITION BY user_id, session_id ORDER BY timestamp)* to ensure the correct ordering of events within each session.\n",
    "\n",
    "3. CTE **filtered_events**:\n",
    "- Removes consecutive duplicate *event_page* entries.\n",
    "- For each session: if the *event_page* is different from the *prev_page*, it's kept in the result. If *prev_page* is *NULL* (which happens for the first event is a session), that event is also kept.\n",
    "- This ensures that only unique page transitions are included in the route, avoiding consecutive duplicates.\n",
    "\n",
    "4. CTE **all_routes**:\n",
    "- Creates a \"route\" for each session by concatenating the pages visited in the correct order.\n",
    "- First, the query orders the filtered events by *user_id, session_id* and *timestamp* to ensure that the pages are in the correct sequence.\n",
    "- Then, it uses *collect_list(event_page)* to gather all *event_page* values for each *(user_id, session_id)* pair and concatenate them into a single string using *concat_ws('-')*, which creates the route.\n",
    "- This CTE produces the user route, which is a sequence of pages visited during each session.\n",
    "\n",
    "5. Final query: **Most Frequent Routes**:\n",
    "- Counts the number of distinct *(user_id, session_id)* pairs (i.e., unique sessions) for each route and returns the top 30 most frequent routes.\n",
    "- *count(distinct user_id, session_id)* counts how many unique sessions had each specific route.\n",
    "- The query then groups the results by *route* and orders by the count in descending order.\n",
    "- *limit 30* ensures that only the top 30 routes are returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Let's also include here a version without cte, just for fun:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main\t8184\n",
      "main-archive\t1113\n",
      "main-rabota\t1047\n",
      "main-internet\t897\n",
      "main-bonus\t870\n",
      "main-news\t769\n",
      "main-tariffs\t677\n",
      "main-online\t587\n",
      "main-vklad\t518\n",
      "main-rabota-archive\t170\n",
      "main-archive-rabota\t167\n",
      "main-bonus-archive\t143\n",
      "main-rabota-bonus\t139\n",
      "main-bonus-rabota\t135\n",
      "main-news-rabota\t135\n",
      "main-archive-internet\t132\n",
      "main-rabota-news\t130\n",
      "main-internet-rabota\t129\n",
      "main-archive-news\t126\n",
      "main-rabota-internet\t124\n",
      "main-internet-archive\t123\n",
      "main-archive-bonus\t117\n",
      "main-internet-bonus\t115\n",
      "main-tariffs-internet\t114\n",
      "main-news-archive\t113\n",
      "main-news-internet\t109\n",
      "main-archive-tariffs\t104\n",
      "main-internet-news\t103\n",
      "main-tariffs-archive\t103\n",
      "main-rabota-main\t94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "most_frequent_routes_without_cte = se.sql('''\n",
    "select route, count(distinct user_id, session_id) as cnt\n",
    "from (\n",
    "    select user_id, session_id, concat_ws(\"-\", collect_list(event_page)) as route\n",
    "    from (\n",
    "        select user_id, session_id, event_page, timestamp\n",
    "        from (\n",
    "            select d.user_id, d.session_id, d.event_page, d.timestamp,\n",
    "                   lag(d.event_page) over (partition by d.user_id, d.session_id order by d.timestamp) as prev_page\n",
    "            from df d\n",
    "            left join (\n",
    "                select user_id, session_id, min(timestamp) as first_error_timestamp\n",
    "                from df\n",
    "                where event_type like '%error%'\n",
    "                group by user_id, session_id\n",
    "            ) sl on sl.user_id = d.user_id and sl.session_id = d.session_id\n",
    "            where d.event_type = 'page' \n",
    "              and (sl.first_error_timestamp is null or d.timestamp < sl.first_error_timestamp)\n",
    "            order by d.user_id, d.session_id, d.timestamp\n",
    "        ) \n",
    "        where event_page != prev_page or prev_page is null\n",
    "    )\n",
    "    group by user_id, session_id\n",
    ") \n",
    "group by route\n",
    "order by cnt desc, route\n",
    "limit 30\n",
    "''')\n",
    "\n",
    "most_frequent_routes_spark_sql_without_cte = most_frequent_routes_without_cte.collect()\n",
    "\n",
    "for row in most_frequent_routes_spark_sql_without_cte:\n",
    "    print(f\"{row['route']}\\t{row['cnt']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Now, we should create Spark RDD Solution and Spak DF solution using the same logic and compare the results:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Spark RDD Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rdd = df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:==================================================>     (29 + 2) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main\t8184\n",
      "main-archive\t1113\n",
      "main-rabota\t1047\n",
      "main-internet\t897\n",
      "main-bonus\t870\n",
      "main-news\t769\n",
      "main-tariffs\t677\n",
      "main-online\t587\n",
      "main-vklad\t518\n",
      "main-rabota-archive\t170\n",
      "main-archive-rabota\t167\n",
      "main-bonus-archive\t143\n",
      "main-rabota-bonus\t139\n",
      "main-bonus-rabota\t135\n",
      "main-news-rabota\t135\n",
      "main-archive-internet\t132\n",
      "main-rabota-news\t130\n",
      "main-internet-rabota\t129\n",
      "main-archive-news\t126\n",
      "main-rabota-internet\t124\n",
      "main-internet-archive\t123\n",
      "main-archive-bonus\t117\n",
      "main-internet-bonus\t115\n",
      "main-tariffs-internet\t114\n",
      "main-news-archive\t113\n",
      "main-news-internet\t109\n",
      "main-archive-tariffs\t104\n",
      "main-internet-news\t103\n",
      "main-tariffs-archive\t103\n",
      "main-rabota-main\t94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "session_limit_rdd = (df_rdd\n",
    "    .filter(lambda row: 'error' in row['event_type'])\n",
    "    .map(lambda row: ((row['user_id'], row['session_id']), row['timestamp']))\n",
    "    .reduceByKey(min)\n",
    ")\n",
    "\n",
    "valid_events_rdd = (df_rdd\n",
    "    .map(lambda row: ((row['user_id'], row['session_id']), row))\n",
    "    .leftOuterJoin(session_limit_rdd)\n",
    "    .filter(lambda x: x[1][0]['event_type'] == 'page' and \n",
    "                      (x[1][1] is None or x[1][0]['timestamp'] < x[1][1]))\n",
    ")\n",
    "\n",
    "def remove_consecutive_duplicates(session):\n",
    "    ordered_session = sorted(session, key=lambda x: x[1])\n",
    "    result = []\n",
    "    prev_page = None\n",
    "    for page, timestamp in ordered_session:\n",
    "        if page != prev_page:\n",
    "            result.append((page, timestamp))\n",
    "        prev_page = page\n",
    "    return result\n",
    "\n",
    "filtered_events_rdd = (valid_events_rdd\n",
    "    .map(lambda x: ((x[0][0], x[0][1]), (x[1][0]['event_page'], x[1][0]['timestamp'])))\n",
    "    .groupByKey()\n",
    "    .mapValues(remove_consecutive_duplicates)\n",
    ")\n",
    "\n",
    "routes_rdd = (filtered_events_rdd\n",
    "    .map(lambda x: (x[0], '-'.join([page for page, _ in x[1]])))\n",
    ")\n",
    "\n",
    "most_frequent_routes_rdd = (routes_rdd\n",
    "    .map(lambda x: (x[1], (x[0][0], x[0][1])))  \n",
    "    .distinct()\n",
    "    .map(lambda x: (x[0], 1))  \n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .sortBy(lambda x: (-x[1], x[0]))\n",
    "    .take(30)\n",
    ")\n",
    "\n",
    "for route, cnt in most_frequent_routes_rdd:\n",
    "    print(f\"{route}\\t{cnt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Spark DF Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main\t8184\n",
      "main-archive\t1113\n",
      "main-rabota\t1047\n",
      "main-internet\t897\n",
      "main-bonus\t870\n",
      "main-news\t769\n",
      "main-tariffs\t677\n",
      "main-online\t587\n",
      "main-vklad\t518\n",
      "main-rabota-archive\t170\n",
      "main-archive-rabota\t167\n",
      "main-bonus-archive\t143\n",
      "main-rabota-bonus\t139\n",
      "main-bonus-rabota\t135\n",
      "main-news-rabota\t135\n",
      "main-archive-internet\t132\n",
      "main-internet-rabota\t129\n",
      "main-rabota-news\t129\n",
      "main-archive-news\t126\n",
      "main-rabota-internet\t124\n",
      "main-internet-archive\t123\n",
      "main-archive-bonus\t117\n",
      "main-internet-bonus\t115\n",
      "main-tariffs-internet\t114\n",
      "main-news-archive\t113\n",
      "main-news-internet\t109\n",
      "main-archive-tariffs\t104\n",
      "main-internet-news\t103\n",
      "main-tariffs-archive\t103\n",
      "main-rabota-main\t94\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "session_limit = (df\n",
    "    .filter(F.col(\"event_type\").like(\"%error%\"))\n",
    "    .groupBy(\"user_id\", \"session_id\")\n",
    "    .agg(F.min(\"timestamp\").alias(\"first_error_timestamp\"))\n",
    ")\n",
    "\n",
    "window_spec = Window.partitionBy(\"user_id\", \"session_id\").orderBy(\"timestamp\")\n",
    "ordered_events = (df\n",
    "    .join(session_limit, on=[\"user_id\", \"session_id\"], how=\"left\")\n",
    "    .filter((F.col(\"event_type\") == \"page\") & \n",
    "            ((F.col(\"first_error_timestamp\").isNull()) | (F.col(\"timestamp\") < F.col(\"first_error_timestamp\"))))\n",
    "    .withColumn(\"prev_page\", F.lag(\"event_page\").over(window_spec))\n",
    ")\n",
    "\n",
    "filtered_events = ordered_events.filter((F.col(\"event_page\") != F.col(\"prev_page\")) | F.col(\"prev_page\").isNull())\n",
    "\n",
    "filtered_events_ordered = filtered_events.orderBy(\"user_id\", \"session_id\", \"timestamp\")\n",
    "\n",
    "all_routes = (filtered_events_ordered\n",
    "    .groupBy(\"user_id\", \"session_id\")\n",
    "    .agg(F.concat_ws(\"-\", F.collect_list(\"event_page\")).alias(\"route\"))\n",
    ")\n",
    "\n",
    "most_frequent_routes_df = (all_routes\n",
    "    .groupBy(\"route\")\n",
    "    .agg(F.countDistinct(\"user_id\", \"session_id\").alias(\"cnt\"))\n",
    "    .orderBy(F.col(\"cnt\").desc(), F.col(\"route\"))\n",
    "    .limit(30)\n",
    ")\n",
    "\n",
    "most_frequent_routes_df = most_frequent_routes_df.collect()\n",
    "\n",
    "for row in most_frequent_routes_df:\n",
    "    print(f'{row[\"route\"]}\\t{row[\"cnt\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Let's save top 10 to result.json**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 10 most frequent routes have been saved to result.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "routes_dict = {row['route']: row['cnt'] for row in most_frequent_routes_spark_sql[:10]}\n",
    "\n",
    "with open(\"result.json\", \"w\") as f:\n",
    "    json.dump(routes_dict, f, indent=4)\n",
    "\n",
    "print(\"The top 10 most frequent routes have been saved to result.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **And compare our results:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 30 most frequent user routes:\n",
      "Route (Spark SQL)         Count      | Route (RDD)               Count      | Route (DataFrame)         Count     \n",
      "main                      8184       | main                      8184       | main                      8184      \n",
      "main-archive              1113       | main-archive              1113       | main-archive              1113      \n",
      "main-rabota               1047       | main-rabota               1047       | main-rabota               1047      \n",
      "main-internet             897        | main-internet             897        | main-internet             897       \n",
      "main-bonus                870        | main-bonus                870        | main-bonus                870       \n",
      "main-news                 769        | main-news                 769        | main-news                 769       \n",
      "main-tariffs              677        | main-tariffs              677        | main-tariffs              677       \n",
      "main-online               587        | main-online               587        | main-online               587       \n",
      "main-vklad                518        | main-vklad                518        | main-vklad                518       \n",
      "main-rabota-archive       170        | main-rabota-archive       170        | main-rabota-archive       170       \n",
      "main-archive-rabota       167        | main-archive-rabota       167        | main-archive-rabota       167       \n",
      "main-bonus-archive        143        | main-bonus-archive        143        | main-bonus-archive        143       \n",
      "main-rabota-bonus         139        | main-rabota-bonus         139        | main-rabota-bonus         139       \n",
      "main-bonus-rabota         135        | main-bonus-rabota         135        | main-bonus-rabota         135       \n",
      "main-news-rabota          135        | main-news-rabota          135        | main-news-rabota          135       \n",
      "main-archive-internet     132        | main-archive-internet     132        | main-archive-internet     132       \n",
      "main-rabota-news          130        | main-rabota-news          130        | main-internet-rabota      129       \n",
      "main-internet-rabota      129        | main-internet-rabota      129        | main-rabota-news          129       \n",
      "main-archive-news         126        | main-archive-news         126        | main-archive-news         126       \n",
      "main-rabota-internet      124        | main-rabota-internet      124        | main-rabota-internet      124       \n",
      "main-internet-archive     123        | main-internet-archive     123        | main-internet-archive     123       \n",
      "main-archive-bonus        117        | main-archive-bonus        117        | main-archive-bonus        117       \n",
      "main-internet-bonus       115        | main-internet-bonus       115        | main-internet-bonus       115       \n",
      "main-tariffs-internet     114        | main-tariffs-internet     114        | main-tariffs-internet     114       \n",
      "main-news-archive         113        | main-news-archive         113        | main-news-archive         113       \n",
      "main-news-internet        109        | main-news-internet        109        | main-news-internet        109       \n",
      "main-archive-tariffs      104        | main-archive-tariffs      104        | main-archive-tariffs      104       \n",
      "main-internet-news        103        | main-internet-news        103        | main-internet-news        103       \n",
      "main-tariffs-archive      103        | main-tariffs-archive      103        | main-tariffs-archive      103       \n",
      "main-rabota-main          94         | main-rabota-main          94         | main-rabota-main          94        \n"
     ]
    }
   ],
   "source": [
    "combined_routes = zip(\n",
    "    most_frequent_routes_spark_sql, \n",
    "    most_frequent_routes_rdd, \n",
    "    most_frequent_routes_df\n",
    ")\n",
    "\n",
    "print(\"The 30 most frequent user routes:\")\n",
    "print(f\"{'Route (Spark SQL)':<25} {'Count':<10} | {'Route (RDD)':<25} {'Count':<10} | {'Route (DataFrame)':<25} {'Count':<10}\")\n",
    "\n",
    "for row_sql, row_rdd, row_df in combined_routes:\n",
    "    print(f\"{row_sql['route']:<25} {row_sql['cnt']:<10} | {row_rdd[0]:<25} {row_rdd[1]:<10} | {row_df['route']:<25} {row_df['cnt']:<10}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that, for the most part, our results are consistent. However, there are occasional slight discrepancies between them.\n",
    "\n",
    "- For example: main-rabota-news 130 | main-rabota-news 130 | main-internet-rabota 129\n",
    "\n",
    "This variation can be attributed to the format of the timestamp field, which is represented as Unix timestamps in seconds. Since multiple pages can sometimes be visited at the same second, the order of events during the *order by* operation can become unpredictable, leading to different routes being generated in various instances.\n",
    "\n",
    "If the timestamp were recorded in milliseconds, these discrepancies might not occur, as the more precise timing would better reflect the actual sequence of events.\n",
    "\n",
    "Overall, despite these minor differences, our results appear to be reasonable and consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "week-4-spark-homework"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
